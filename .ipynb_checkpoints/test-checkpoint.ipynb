{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# make sure to use position 1\n",
    "sys.path.insert(1, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = np.loadtxt('/media/yonsei/4TB_HDD/0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File('/media/yonsei/4TB_HDD/0.hdf5', 'w')\n",
    "data_file.create_dataset('group_name', data=dataFile)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 64)\n",
      "0.0799810886383\n",
      "(4096, 64)\n",
      "0.0415949821472\n",
      "(4096, 64)\n",
      "0.0544030666351\n",
      "(4096, 64)\n",
      "0.340887069702\n"
     ]
    }
   ],
   "source": [
    "dataFile = '/media/yonsei/4TB_HDD/0.txt'\n",
    "startTime = time.time()\n",
    "df=pd.read_csv(dataFile, delim_whitespace=True, header=None)\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime\n",
    "\n",
    "dataFile = '/media/yonsei/4TB_HDD/0.txt'\n",
    "startTime = time.time()\n",
    "df=pd.read_csv(dataFile, delimiter=' ', dtype=float, header=None)\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime\n",
    "\n",
    "dataFile = '/media/yonsei/4TB_HDD/0.txt'\n",
    "startTime = time.time()\n",
    "df=pd.read_table(dataFile,delim_whitespace=True, header=None)\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime\n",
    "\n",
    "dataFile = '/media/yonsei/4TB_HDD/0.hdf5'\n",
    "startTime = time.time()\n",
    "f = h5py.File(dataFile, 'r')\n",
    "a_group_key = list(f.keys())[0]\n",
    "df = list(f[a_group_key])\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 64)\n",
      "0.0396058559418\n",
      "<type 'numpy.int64'>\n",
      "(4096, 64)\n",
      "0.0318930149078\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "dataFile = '/media/yonsei/4TB_HDD/0.txt'\n",
    "startTime = time.time()\n",
    "df=pd.read_csv(dataFile, delim_whitespace=True, header=None)\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime\n",
    "df = np.array(df)\n",
    "print type(df[0,0])\n",
    "\n",
    "dataFile = '/media/yonsei/4TB_HDD/0.txt'\n",
    "startTime = time.time()\n",
    "df=pd.read_csv(dataFile, delim_whitespace=True, dtype=float, header=None)\n",
    "print np.array(df).shape\n",
    "print time.time() - startTime\n",
    "df = np.array(df)\n",
    "print type(df[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.io.parsers.TextFileReader object at 0x7f340fecf090>\n"
     ]
    }
   ],
   "source": [
    "print np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import os, pickle\n",
    "import src.module.nolbo as nolboModule\n",
    "import dataset_utils.dataset_loader.nolbo as nolboDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nolboConfig = {\n",
    "    'inputImgDim':[416,416,1],\n",
    "    'maxPoolNum':5,\n",
    "    'predictorNumPerGrid':2,\n",
    "    'bboxDim':5,\n",
    "    'class':True, 'zClassDim':64, 'classDim':30,\n",
    "    'inst':True, 'zInstDim':64, 'instDim':1300,\n",
    "    'rot':True, 'zRotDim':3, 'rotDim':3,\n",
    "    'trainable':True,\n",
    "    'decoderStructure':{\n",
    "        'outputImgDim':[64,64,64,1],\n",
    "        'trainable':True,    \n",
    "        'filterNumList':[512,256,128,64,1],\n",
    "        'kernelSizeList':[4,4,4,4,4],\n",
    "        'stridesList':[1,2,2,2,2],\n",
    "        'activation':tf.nn.leaky_relu,\n",
    "        'lastLayerActivation':tf.nn.sigmoid\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = nolboModule.darknet_classifier(\n",
    "    batchSize=36, imgSize=(416,416), lastLayerActivation=tf.nn.sigmoid,\n",
    "    dataPath='./data/classifier_training_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(epoch=10, weightSavePath='./weights/classifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data path...\n",
      "2661 houseId : 2d5ff998441abd9bda008cebc9949f65 \n",
      "data path shuffle...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "dataset = nolboDataset.nolboDataset(nolboConfig=nolboConfig, mode='singleObject', datasetPath='/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/0a0b9b45a1db29832dd84e80c1347854/room-fr_0rm_0-0/voxel3D/objInfo_0/0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.3748250008\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(10):\n",
    "    fuck = dataset.getNextBatch(32)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.2895858288\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(10):\n",
    "    fuck = dataset.getNextBatch(32)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.14395713806\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "a = []\n",
    "b = []\n",
    "for i in range(320):\n",
    "    a.append(None)\n",
    "for i in range(320):\n",
    "    a[i] = pandas.read_csv(os.path.join('/media/yonsei/4TB_HDD/', str(0)+'.txt'), delimiter=' ', dtype=float, header=None)\n",
    "    b.append(a[i])\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in batchData.keys():\n",
    "    print key, batchData[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outputImg in batchData['outputImages']:\n",
    "    print outputImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(320):\n",
    "    a = np.array(pandas.read_csv(os.path.join('/media/yonsei/4TB_HDD/', str(0) + '.txt'), delimiter=' ', dtype=float, header=None))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data path...\n",
      "2661 2d5ff998441abd9bda008cebc9949f65 \n",
      "data path shuffle...\n",
      "done!\n",
      "init training...\n",
      "done!\n",
      "build network...\n",
      "encoder_singleVectorOutput - nolbo-enc\n",
      "darknet19_core - nolbo-encDKCore\n",
      "(?, 416, 416, 32)\n",
      "(?, 208, 208, 32)\n",
      "(?, 208, 208, 64)\n",
      "(?, 104, 104, 64)\n",
      "(?, 104, 104, 128)\n",
      "(?, 104, 104, 64)\n",
      "(?, 104, 104, 128)\n",
      "(?, 52, 52, 128)\n",
      "(?, 52, 52, 256)\n",
      "(?, 52, 52, 128)\n",
      "(?, 52, 52, 256)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "encoder_singleVectorOutput_lastLayer - nolbo-enc\n",
      "(?, 262)\n",
      "decoder - nolbo-dec\n",
      "(?, 131)\n",
      "(?, 512)\n",
      "(?, 512)\n",
      "(?, 4, 4, 4, 8)\n",
      "(?, 4, 4, 4, 512)\n",
      "(?, 8, 8, 8, 256)\n",
      "(?, 16, 16, 16, 128)\n",
      "(?, 32, 32, 32, 64)\n",
      "(?, 64, 64, 64, 1)\n",
      "done!\n",
      "create loss...\n",
      "set optimizer...\n",
      "INFO:tensorflow:Restoring parameters from weights/classifier/nolbo_encoderCore.ckpt\n",
      "start training...\n",
      "Epoch:00001 iter:02001 runtime:3.208 curr/total:20287/50001 loss= total:7855.248, voxel:7855.24889\n",
      "save model...\n",
      "Epoch:00001 iter:02001 runtime:3.214 curr/total:40567/50001 loss= total:7334.257, voxel:7334.257\n",
      "save model...\n",
      "data path shuffle...35 runtime:3.160 curr/total:49971/50001 loss= total:7184.909, voxel:7184.909\n",
      "done!\n",
      "\n",
      "Epoch:00002 iter:02001 runtime:3.054 curr/total:20233/50001 loss= total:7114.249, voxel:7114.249\n",
      "save model...\n",
      "Epoch:00002 iter:02001 runtime:3.146 curr/total:40434/50001 loss= total:7012.417, voxel:7012.417\n",
      "save model...\n",
      "Epoch:00002 iter:00168 runtime:3.134 curr/total:42140/50001 loss= total:6915.172, voxel:6915.172\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56874fb10361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdatasetPath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msavePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weights/nolbo_singleObject/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrestorePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weights/classifier/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-4-141c6f05b673>\u001b[0m in \u001b[0;36mtrainNolboSingleObject\u001b[0;34m(nolboConfig, batchSize, training_epoch, datasetPath, savePath, restorePath)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mtraining_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mbatchData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNextBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mepochCurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdataStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataStart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yonsei/pyws2/NOLBO/dataset_utils/dataset_loader/nolbo.pyc\u001b[0m in \u001b[0;36mgetNextBatch\u001b[0;34m(self, batchSize)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNextBatchMultiObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximumBatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'singleObject'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNextBatchSingleObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"mode should be 'multiObject' or singleObject'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yonsei/pyws2/NOLBO/dataset_utils/dataset_loader/nolbo.pyc\u001b[0m in \u001b[0;36m_getNextBatchSingleObject\u001b[0;34m(self, batchSize)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0;31m# print inputImage.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0mimageCanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageCanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nolboConfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputImgDim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mimageCanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imageAugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageCanvas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mobjClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjInst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objClassAndInst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobjShapePath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yonsei/pyws2/NOLBO/dataset_utils/dataset_loader/nolbo.pyc\u001b[0m in \u001b[0;36m_imageAugmentation\u001b[0;34m(self, inputImages)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;31m# print noiseTypeList[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0minputImages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputImages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_typ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoiseTypeList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yonsei/pyws2/NOLBO/dataset_utils/datasetUtils.pyc\u001b[0m in \u001b[0;36mnoisy\u001b[0;34m(image, noise_typ)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnoise_typ\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"speckle\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mgauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mnoisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainNolboSingleObject(\n",
    "    nolboConfig=nolboConfig,\n",
    "    batchSize=32,\n",
    "    training_epoch = 1000,\n",
    "    datasetPath='/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/',\n",
    "    savePath='weights/nolbo_singleObject/',\n",
    "    restorePath='weights/classifier/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNolboSingleObject(\n",
    "    nolboConfig, \n",
    "    batchSize, training_epoch, \n",
    "    datasetPath=None,\n",
    "    savePath = None, restorePath=None):\n",
    "    \n",
    "    dataset = nolboDataset.nolboDataset(nolboConfig=nolboConfig, mode='singleObject', datasetPath=datasetPath)\n",
    "    nb = nolboModule.nolbo_singleObject(config=nolboConfig)\n",
    "    if restorePath!=None:\n",
    "        nb.restoreEncoderCore(restorePath)\n",
    "    loss = np.zeros(2)\n",
    "    epoch = 0\n",
    "    iteration = 0\n",
    "    run_time = 0.0\n",
    "    print 'start training...'\n",
    "    while epoch<training_epoch:\n",
    "        start = time.time()\n",
    "        batchData = dataset.getNextBatch(batchSize=batchSize)\n",
    "        epochCurr = dataset._epoch\n",
    "        dataStart = dataset._dataStart\n",
    "        dataLength = dataset._dataLength\n",
    "        if epochCurr!=epoch:\n",
    "            print ''\n",
    "            iteration = 0\n",
    "            loss = loss*0.0\n",
    "            runTime = 0.0\n",
    "        epoch = epochCurr\n",
    "        lossTemp = np.array(nb.fit(batchDict=batchData))\n",
    "        end = time.time()\n",
    "        loss = (loss*iteration + lossTemp)/(iteration+1.0)\n",
    "        run_time = (run_time*iteration + (end-start))/(iteration + 1.0)\n",
    "\n",
    "        print 'Epoch:{:05d} iter:{:05d} runtime:{:.3f}'.format(int(epoch+1), int(iteration+1), run_time),\\\n",
    "        'curr/total:{:05d}/{:05d}'.format(dataStart,dataLength), \\\n",
    "        'loss= total:{:.3f}, voxel:{:.3f}\\r'.format(loss[0],loss[1]),\n",
    "\n",
    "        if iteration%2000 == 0 and iteration!=0:\n",
    "            print ''\n",
    "            iteration = 0\n",
    "            loss = loss*0.0\n",
    "            runTime = 0.0\n",
    "            if savePath!=None:\n",
    "                print 'save model...'\n",
    "                nb.saveNetworks(savePath)\n",
    "        iteration = iteration + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNolbo(nolboConfig=nolboConfig, maxBatchSize=32, training_epoch=10000, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNolbo(nolboConfig, maxBatchSize, training_epoch, restore=False):\n",
    "    dataset = nolboDataset.nolboDataset(nolboConfig=nolboConfig, datasetPath='/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/')\n",
    "    nb = nolboModule.nolbo_multiObject(config=nolboConfig)\n",
    "    if restore == True:\n",
    "        vae.restore_model('./weight_m40')\n",
    "    loss = np.zeros(5)\n",
    "    epoch = 0\n",
    "    iteration = 0\n",
    "    run_time = 0.0\n",
    "    print 'start training...'\n",
    "    while epoch<training_epoch:\n",
    "        start = time.time()\n",
    "        batchData = dataset.getNextBatch(maximumBatchSize=maxBatchSize)\n",
    "        epochCurr = dataset._epoch\n",
    "        dataStart = dataset._dataStart\n",
    "        dataLength = dataset._dataLength\n",
    "        if epochCurr != epoch:\n",
    "            iteration = 0\n",
    "            loss = loss * 0.0\n",
    "            run_time = 0.0\n",
    "        epoch = epochCurr\n",
    "#         print np.sum(batchData[\"objectness\"])\n",
    "        lossTemp = np.array(nb.fit(batchDict = batchData))\n",
    "        end = time.time()\n",
    "        loss = (loss*iteration + lossTemp)/(iteration+1.0)\n",
    "        run_time = (run_time*iteration + (end-start))/(iteration + 1.0)\n",
    "\n",
    "        print 'Epoch:{:05d} iter:{:05d} runtime:{:.3f}'.format(int(epoch+1), int(iteration+1), run_time),\\\n",
    "        'curr/total:{:05d}/{:05d}'.format(dataStart,dataLength), \\\n",
    "        'loss= total:{:.3f}, bbox:{:.3f}, obj:{:.3f}, noObj:{:.3f}, voxel:{:.3f}\\r'.format(loss[0],loss[1],loss[2],loss[3],loss[4]),\n",
    "        if dataStart+maxBatchSize >= dataLength:\n",
    "            print ''\n",
    "            if iteration%2000 == 0:\n",
    "                print 'save model No.'+str(int(iteration%2000)+1)\n",
    "                vae.save_model('./weight_m40')\n",
    "        iteration = iteration + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainNolbo(nolboConfig=nolboConfig, maxBatchSize=32, training_epoch=10000, restore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = nolboModule.nolbo_multiObject(config=nolboConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nolboDataset.nolboDataset(nolboConfig=nolboConfig, datasetPath='/media/yonsei/4TB_HDD/dataset/suncg_data/training_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataset.getNextBatch(maximumBatchSize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print a.keys()\n",
    "objectness = a['objectness']\n",
    "bboxHWXY = a['bboxHWXY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print objness.shape\n",
    "print bboxHWXY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuck = np.tile(objness, (1,1,1,2,1))\n",
    "print fuck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (10*bboxHWXY[0,:,:,0,:]).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(2,2,3)\n",
    "print a\n",
    "a = a.reshape((2,2,3))\n",
    "inputs = tf.placeholder(tf.float32, shape=(2,2,3))\n",
    "_, indices = tf.nn.top_k(inputs,3)\n",
    "indices = tf.reshape(indices,(-1,3))\n",
    "indices = tf.map_fn(tf.invert_permutation, indices)\n",
    "indices = tf.reshape(indices, (-1,2,3))\n",
    "mask = tf.where(indices<=0, tf.ones_like(indices), tf.zeros_like(indices))\n",
    "sess = tf.Session()\n",
    "b = sess.run(mask, {inputs:a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.transpose(np.reshape(np.array(\n",
    "            [np.arange(13)] * 13 * 2),\n",
    "(2, 13, 13)), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print a[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = np.transpose(a, [1,0,2])\n",
    "print at[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
