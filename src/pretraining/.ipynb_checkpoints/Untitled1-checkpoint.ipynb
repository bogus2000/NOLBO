{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class darknet19_core(object):\n",
    "    def __init__(self, nameScope='dartnet19_core', trainable=True, bnPhase=True, reuse=False, activation = tf.nn.leaky_relu):\n",
    "        self._reuse = reuse\n",
    "        self._trainable = trainable\n",
    "        self._bnPhase = bnPhase\n",
    "        self._nameScope = nameScope\n",
    "        self._activation = activation\n",
    "        self.variables = None\n",
    "        self.update_ops = None\n",
    "        self.saver = None\n",
    "    def _conv(self, inputs, filters, kernel_size):\n",
    "        hiddenC = tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=1, padding='same', activation=None, trainable=self._trainable, use_bias=False)\n",
    "        hiddenC = tf.layers.batch_normalization(inputs=hiddenC, training=self._bnPhase, trainable=self._trainable)\n",
    "        hiddenC = self._activation(hiddenC)\n",
    "        print hiddenC.shape\n",
    "        return hiddenC\n",
    "    def _maxPool(self, inputs, pool_size=(2,2), strides=2, padding='same'):\n",
    "        hiddenP = tf.layers.max_pooling2d(inputs, pool_size=pool_size, strides=strides, padding=padding)\n",
    "        print hiddenP.shape\n",
    "        return hiddenP\n",
    "    def __call__(self, inputImg):\n",
    "        with tf.variable_scope(self._nameScope, reuse=self._reuse):\n",
    "            hiddenC1 = self._conv(inputs=inputImg, filters=32, kernel_size=3)\n",
    "            hiddenP1 = self._maxPool(inputs=hiddenC1)\n",
    "            \n",
    "            hiddenC2 = self._conv(inputs=hiddenP1, filters=64, kernel_size=3)\n",
    "            hiddenP2 = self._maxPool(inputs=hiddenC2)\n",
    "            \n",
    "            hiddenC31 = self._conv(inputs=hiddenP2, filters=128, kernel_size=3)\n",
    "            hiddenC32 = self._conv(inputs=hiddenC31, filters=64, kernel_size=1)\n",
    "            hiddenC33 = self._conv(inputs=hiddenC32, filters=128, kernel_size=3)\n",
    "            hiddenP3 = self._maxPool(inputs=hiddenC33)\n",
    "            \n",
    "            hiddenC41 = self._conv(inputs=hiddenP3, filters=256, kernel_size=3)\n",
    "            hiddenC42 = self._conv(inputs=hiddenC41, filters=128, kernel_size=1)\n",
    "            hiddenC43 = self._conv(inputs=hiddenC42, filters=256, kernel_size=3)\n",
    "            hiddenP4 = self._maxPool(inputs=hiddenC43)\n",
    "            \n",
    "            hiddenC51 = self._conv(inputs=hiddenP4, filters=512, kernel_size=3)\n",
    "            hiddenC52 = self._conv(inputs=hiddenC51, filters=256, kernel_size=1)\n",
    "            hiddenC53 = self._conv(inputs=hiddenC52, filters=512, kernel_size=3)\n",
    "            hiddenC54 = self._conv(inputs=hiddenC53, filters=256, kernel_size=1)\n",
    "            hiddenC55 = self._conv(inputs=hiddenC54, filters=512, kernel_size=3)\n",
    "            hiddenP5 = self._maxPool(inputs=hiddenC55)\n",
    "            \n",
    "            hiddenC61 = self._conv(inputs=hiddenP5, filters=1024, kernel_size=3)\n",
    "            hiddenC62 = self._conv(inputs=hiddenC61, filters=512, kernel_size=1)\n",
    "            hiddenC63 = self._conv(inputs=hiddenC62, filters=1024, kernel_size=3)\n",
    "            hiddenC64 = self._conv(inputs=hiddenC63, filters=512, kernel_size=1)\n",
    "            hiddenC65 = self._conv(inputs=hiddenC64, filters=1024, kernel_size=3)\n",
    "        self._reuse = True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._nameScope)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self._nameScope)\n",
    "        self.saver = tf.train.Saver(var_list=self.variables)\n",
    "        outputs = hiddenC65\n",
    "        return outputs\n",
    "class darknet19_pretraining(object):\n",
    "    def __init__(self, classNum, nameScope='darknet19_pretraining', trainable=True, bnPhase=True, reuse=False, activation = tf.nn.leaky_relu):\n",
    "        self._classNum = classNum\n",
    "        self._nameScope = nameScope\n",
    "        self._trainable = trainable\n",
    "        self._bnPhase = bnPhase\n",
    "        self._reuse = reuse\n",
    "        self._activation = activation\n",
    "        self.variables = None\n",
    "        self.update_ops = None\n",
    "        self.saver = None\n",
    "    def __call__(self, inputImg):\n",
    "        with tf.variable_scope(self._nameScope, reuse=self._reuse):\n",
    "            hiddenC1 = tf.layers.conv2d(inputs=inputImg, filters=self._classNum, kernel_size=1, strides=1, padding='same', activation=None, trainable=self._trainable, use_bias=True)\n",
    "            hiddenP1 = tf.reduce_mean(hiddenC1, axis=[1,2])\n",
    "            hiddenB1 = tf.nn.sigmoid(hiddenP1)\n",
    "            print hiddenB1.shape\n",
    "        self._reuse=True\n",
    "        self.variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self._nameScope)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self._nameScope)\n",
    "        self.saver = tf.train.Saver(var_list=self.variables)\n",
    "        outputs = hiddenB1\n",
    "        return outputs\n",
    "class darknet_classifier(object):\n",
    "    def __init__(self, dataPath='./', imgSize = (416,416), batchSize = 64, learningRate = 0.001):\n",
    "        self._imgList = None\n",
    "        self._imgClassList = None\n",
    "        self._dataPath = dataPath\n",
    "        self._imgSize = imgSize\n",
    "        self._batchSize = batchSize\n",
    "        self._lr = learningRate\n",
    "        self._classNum = None\n",
    "        self.variables = None\n",
    "        self.update_ops = None\n",
    "        self._inputImg = None\n",
    "        self._outputClass = None\n",
    "        self._outputClassGT = None\n",
    "        self._optimizer = None\n",
    "        self._loss = None\n",
    "        self._loadDataset()\n",
    "        self._buildNetwork()\n",
    "        self._createLossAndOptimizer()\n",
    "        #init the session\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)\n",
    "        self._sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n",
    "        #initialize variables\n",
    "        init = tf.group(\n",
    "            tf.global_variables_initializer(),\n",
    "            tf.local_variables_initializer()\n",
    "        )\n",
    "        #launch the session\n",
    "        self._sess.run(init)\n",
    "    def _loadDataset(self):\n",
    "        print \"load Dataset...\"\n",
    "        self._imgList = []\n",
    "        imgListTemp = np.load(os.path.join(self._dataPath,'imgList.npy'))\n",
    "        self._imgClassList = np.load(os.path.join(self._dataPath+'imgClassList.npy'))\n",
    "        self._classNum = self._imgClassList.shape[1]\n",
    "        for i in range(len(imgListTemp)):\n",
    "            img = cv2.resize(imgListTemp[i], self._imgSize)\n",
    "            img = img.reshape((self._imgSize[0], self._imgSize[1],1))\n",
    "            self._imgList.append(img)\n",
    "        self._imgList = np.array(self._imgList)\n",
    "        print \"done!\"\n",
    "    def _buildNetwork(self):\n",
    "        print \"build network...\"\n",
    "        self._inputImg = tf.placeholder(tf.float32, shape=(None, self._imgSize[0], self._imgSize[1], 1))\n",
    "        self._outputClassGT = tf.placeholder(tf.float32, shape=(None, self._classNum))\n",
    "        self._darknetCore = darknet19_core()\n",
    "        self._pretraining = darknet19_pretraining(self._classNum)\n",
    "        coreOutput = self._darknetCore(self._inputImg)\n",
    "        self._outputClass = self._pretraining(coreOutput)\n",
    "        print \"done!\"\n",
    "    def _createLossAndOptimizer(self):\n",
    "        print \"create loss and optimizer...\"\n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate=self._lr)\n",
    "        def binaryLoss(xPred, xTarget, epsilon=1e-7):\n",
    "            yTarget = xTarget\n",
    "            yPred = tf.clip_by_value(xPred, clip_value_min=epsilon, clip_value_max=1.0-epsilon)\n",
    "            bce_loss = - tf.reduce_sum(yTarget*tf.log(yPred) + (1.0-yTarget)*tf.log(1.0-yPred), axis=-1)\n",
    "            return bce_loss\n",
    "        self._loss = tf.reduce_mean(binaryLoss(xPred=self._outputClass, xTarget=self._outputClassGT))\n",
    "        with tf.control_dependencies(self._darknetCore.update_ops + self._pretraining.update_ops):\n",
    "            self._optimizer = self._optimizer.minimize(\n",
    "                self._loss, var_list = self._darknetCore.variables + self._pretraining.variables\n",
    "            )\n",
    "        print \"done!\"\n",
    "    def _saveNetwork(self, savePath='./'):\n",
    "        dCorePath = os.path.join(savePath,'/dCore.ckpt')\n",
    "        pretrainPath = os.path.join(savePath,'/pretrain.ckpt')\n",
    "        self._darknetCore.saver.save(self._sess, dCorePath)\n",
    "        self._pretraining.saver.save(self._sess, pretrainPath)\n",
    "    def _restoreNetwork(self, restorePath='./'):\n",
    "        dCorePath = os.path.join(restorePath,'/dCore.ckpt')\n",
    "        pretrainPath = os.path.join(restorePath,'/pretrain.ckpt')\n",
    "        self._darknetCore.saver.restore(self._sess, dCorePath)\n",
    "        self._pretraining.saver.restore(self._sess, pretrainPath)\n",
    "    def _fit(self, batchImg, batchClassIndex):\n",
    "        feed_dict = {\n",
    "            self._inputImg : batchImg,\n",
    "            self._outputClassGT : batchClassIndex\n",
    "        }\n",
    "        accAll = (tf.reduce_sum((1-self._outputClass)*(1-self._outputClassGT))+tf.reduce_sum(self._outputClass*self._outputClassGT))\\\n",
    "        /(tf.reduce_sum(self._outputClassGT)+tf.reduce_sum(1-self._outputClassGT))\n",
    "        accPositive = tf.reduce_sum(self._outputClass*self._outputClassGT)/tf.reduce_sum(self._outputClassGT)\n",
    "        _, lossResult = self._sess.run([self._optimizer, self._loss], feed_dict=feed_dict)\n",
    "        accAllResult, accPositiveResult = self._sess.run([accAll, accPositive], feed_dict=feed_dict)\n",
    "        return lossResult, accAllResult, accPositiveResult\n",
    "    def train(self, epoch = 10000, weightSavePath='./'):\n",
    "        currEpoch = 0\n",
    "        dataCompleted = 0\n",
    "        loss = 0\n",
    "        accAll, accPositive = 0, 0\n",
    "        runTime = 0\n",
    "        for i in range(int(epoch/self._batchSize)):\n",
    "            for i in range(int(len(self._imgList)/self._batchSize)):\n",
    "                startTime = time.time()\n",
    "                start = i * self._batchSize\n",
    "                end = np.min((start+self._batchSize, len(self._imgList)))\n",
    "                lossTemp, accAllTemp, accPositiveTemp = self._fit(self._imgList[start:end], self._imgClassList[start:end])\n",
    "                endTime = time.time()\n",
    "                runTimeTemp = endTime - startTime\n",
    "                accAll = float(acc*currEpoch + accAllTemp)/float(currEpoch+1.0)\n",
    "                accPositive = float(accPositive*currEpoch + accPositiveTemp)/float(currEpoch+1.0)\n",
    "                loss = float(loss*currEpoch + lossTemp)/float(currEpoch+1.0)\n",
    "                runTime = float(runTime*currEpoch + runTimeTemp)/(currEpoch+1.0)\n",
    "                sys.stdout.write('Epoch:{:05d} round:{:04d} runtime:{:.3f} '.format(int(currEpoch+1), int(dataCompleted+1), runTime))\n",
    "                sys.stdout.write('curr/total:{:05d}/{:05d} '.format(start, len(self._imgList)))\n",
    "                sys.stdout.write('loss:{:.3f} accAll:{:.3f} accPos:{:.3f}\\r'.format(loss, accAll, accPositive))\n",
    "                currEpoch += 1\n",
    "                if currEpoch%1000 == 0:\n",
    "                    sys.stdout.write('\\nsaveWeights...\\n')\n",
    "                    self._saveNetwork(weightSavePath)\n",
    "            dataCompleted +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Dataset...\n",
      "done!\n",
      "build network...\n",
      "(?, 416, 416, 32)\n",
      "(?, 208, 208, 32)\n",
      "(?, 208, 208, 64)\n",
      "(?, 104, 104, 64)\n",
      "(?, 104, 104, 128)\n",
      "(?, 104, 104, 64)\n",
      "(?, 104, 104, 128)\n",
      "(?, 52, 52, 128)\n",
      "(?, 52, 52, 256)\n",
      "(?, 52, 52, 128)\n",
      "(?, 52, 52, 256)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 26, 26, 256)\n",
      "(?, 26, 26, 512)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "(?, 13, 13, 512)\n",
      "(?, 13, 13, 1024)\n",
      "(?, 24)\n",
      "done!\n",
      "create loss and optimizer...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "darkClassifier = darknet_classifier(batchSize=32, dataPath='./pretraining_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:01000 round:0004 runtime:0.818 curr/total:01152/10291 loss:4.997 acc:0.8896\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f50ed5a1f22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdarkClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-63d1ad9ea283>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mcurrEpoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrEpoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saveNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mdataCompleted\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-63d1ad9ea283>\u001b[0m in \u001b[0;36m_saveNetwork\u001b[0;34m(self, savePath)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdCorePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/dCore.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mpretrainPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/pretrain.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_darknetCore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdCorePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pretraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrainPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restoreNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestorePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: save() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "darkClassifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
